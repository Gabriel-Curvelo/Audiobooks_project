{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audiobooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is from an audiobook application. Logically, it only refers to the audio versions of the books. Each customer in the database has made a purchase at least once, so it is in the database. I am creating a machine learning algorithm based on our available data that can predict whether a customer will buy again from the audiobook company.\n",
    "\n",
    "The main idea is that if a customer is unlikely to return, there is no reason to spend money on advertising for them. If we can focus our efforts only on customers who are likely to convert again, we can make big savings. In addition, the model can identify the most important metrics for a customer to return to. Identifying new customers creates value and growth opportunities.\n",
    "\n",
    "The .csv file contains the data. There are several variables: Customer ID, Book duration in mins_avg (average of all purchases), Book duration in minutes_sum (sum of all purchases), Price paid_avg (average of all purchases), Price paid_sum (sum of all purchases), Review (a Boolean variable), Review (out of 10), Total minutes heard, Completion (from 0 to 1), Support requests (number) and Last visit minus purchase date (in days).\n",
    "\n",
    "So these are the entries (excluding the customer ID, as it is completely arbitrary. It is more like a name than a number).\n",
    "\n",
    "Destinations are a Boolean variable (therefore, 0 or 1). We are taking a 2 year period on our entries and the next 6 months as targets. So, in fact, we are predicting whether: based on the last 2 years of activity and engagement, a customer will convert in the next 6 months. 6 months seems like a reasonable time. If they don't convert after 6 months, chances are they went to a competitor or don't like the audiobook's way of digesting information.\n",
    "\n",
    "This is a classification problem with two classes: I will not buy and I will buy, represented by 0s and 1s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "raw_csv_data = np.loadtxt('Audiobooks_data.csv', delimiter = ',')\n",
    "\n",
    "unscaled_inputs_all = raw_csv_data[:,1:-1]\n",
    "targets_all = raw_csv_data[:,-1]\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_one_targets = int(np.sum(targets_all))\n",
    "zero_targets_counter = 0\n",
    "indices_to_remove = []\n",
    "\n",
    "for i in range(targets_all.shape[0]):\n",
    "    if targets_all[i] ==0:\n",
    "        zero_targets_counter += 1\n",
    "        if zero_targets_counter > num_one_targets:\n",
    "            indices_to_remove.append(i)\n",
    "            \n",
    "unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all, indices_to_remove, axis = 0)\n",
    "targets_equal_priors = np.delete (targets_all, indices_to_remove, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_inputs = preprocessing.scale(unscaled_inputs_equal_priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_indices = np.arange(scaled_inputs.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
    "shuffled_targets = targets_equal_priors[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset into train, validation, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1823.0 3579 0.5093601564682873\n",
      "195.0 447 0.436241610738255\n",
      "219.0 448 0.4888392857142857\n"
     ]
    }
   ],
   "source": [
    "samples_count = shuffled_inputs.shape[0]\n",
    "\n",
    "train_samples_count = int(0.8*samples_count)\n",
    "validation_samples_count = int(0.1*samples_count)\n",
    "test_samples_count = samples_count - train_samples_count - validation_samples_count\n",
    "\n",
    "train_inputs = shuffled_inputs[:train_samples_count]\n",
    "train_targets = shuffled_targets[:train_samples_count]\n",
    "\n",
    "validation_inputs = shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count]\n",
    "validation_targets = shuffled_targets[train_samples_count:train_samples_count+validation_samples_count]\n",
    "\n",
    "test_inputs = shuffled_inputs[train_samples_count+validation_samples_count:]\n",
    "test_targets = shuffled_targets[train_samples_count+validation_samples_count:]\n",
    "\n",
    "print(np.sum(train_targets), train_samples_count, np.sum(train_targets) / train_samples_count)\n",
    "print(np.sum(validation_targets), validation_samples_count, np.sum(validation_targets) / validation_samples_count)\n",
    "print(np.sum(test_targets), test_samples_count, np.sum(test_targets) / test_samples_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the three datasets in *.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('Audiobooks_data_train', inputs=train_inputs, targets=train_targets)\n",
    "np.savez('Audiobooks_data_validation', inputs=validation_inputs, targets=validation_targets)\n",
    "np.savez('Audiobooks_data_test', inputs=test_inputs, targets=test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "npz = np.load('Audiobooks_data_train.npz')\n",
    "\n",
    "train_inputs = npz['inputs'].astype(np.float)\n",
    "\n",
    "train_targets = npz['targets'].astype(np.int)\n",
    "\n",
    "npz = np.load('Audiobooks_data_validation.npz')\n",
    "\n",
    "validation_inputs, validation_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n",
    "\n",
    "npz = np.load('Audiobooks_data_test.npz')\n",
    "\n",
    "test_inputs, test_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "Outline, optimizers, loss, early stopping and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3579 samples, validate on 447 samples\n",
      "Epoch 1/100\n",
      "3579/3579 - 1s - loss: 0.5496 - accuracy: 0.7779 - val_loss: 0.4253 - val_accuracy: 0.8747\n",
      "Epoch 2/100\n",
      "3579/3579 - 0s - loss: 0.3761 - accuracy: 0.8734 - val_loss: 0.3183 - val_accuracy: 0.8904\n",
      "Epoch 3/100\n",
      "3579/3579 - 0s - loss: 0.3226 - accuracy: 0.8785 - val_loss: 0.2901 - val_accuracy: 0.8971\n",
      "Epoch 4/100\n",
      "3579/3579 - 0s - loss: 0.3031 - accuracy: 0.8849 - val_loss: 0.2834 - val_accuracy: 0.8993\n",
      "Epoch 5/100\n",
      "3579/3579 - 0s - loss: 0.2897 - accuracy: 0.8896 - val_loss: 0.2693 - val_accuracy: 0.9083\n",
      "Epoch 6/100\n",
      "3579/3579 - 0s - loss: 0.2802 - accuracy: 0.8921 - val_loss: 0.2669 - val_accuracy: 0.9128\n",
      "Epoch 7/100\n",
      "3579/3579 - 0s - loss: 0.2734 - accuracy: 0.8963 - val_loss: 0.2637 - val_accuracy: 0.9128\n",
      "Epoch 8/100\n",
      "3579/3579 - 0s - loss: 0.2653 - accuracy: 0.8969 - val_loss: 0.2600 - val_accuracy: 0.9128\n",
      "Epoch 9/100\n",
      "3579/3579 - 0s - loss: 0.2606 - accuracy: 0.9000 - val_loss: 0.2543 - val_accuracy: 0.9128\n",
      "Epoch 10/100\n",
      "3579/3579 - 0s - loss: 0.2573 - accuracy: 0.8997 - val_loss: 0.2537 - val_accuracy: 0.9150\n",
      "Epoch 11/100\n",
      "3579/3579 - 0s - loss: 0.2569 - accuracy: 0.9000 - val_loss: 0.2529 - val_accuracy: 0.9128\n",
      "Epoch 12/100\n",
      "3579/3579 - 0s - loss: 0.2502 - accuracy: 0.9039 - val_loss: 0.2610 - val_accuracy: 0.9150\n",
      "Epoch 13/100\n",
      "3579/3579 - 0s - loss: 0.2463 - accuracy: 0.9058 - val_loss: 0.2543 - val_accuracy: 0.9105\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x25273043bc8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 10\n",
    "output_size = 2\n",
    "\n",
    "hidden_layer_size = 50\n",
    "    \n",
    "model = tf.keras.Sequential([\n",
    "    # tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)\n",
    "    # it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n",
    "    # the final layer make sure to activate it with softmax\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n",
    "])\n",
    "\n",
    "\n",
    "### Optimizer and the loss function\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "### Training\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "max_epochs = 100\n",
    "\n",
    "# early stopping mechanism\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)\n",
    "\n",
    "# fit the model\n",
    "model.fit(train_inputs, # train inputs\n",
    "          train_targets, # train targets\n",
    "          batch_size=batch_size, # batch size\n",
    "          epochs=max_epochs, \n",
    "          callbacks=[early_stopping], # early stopping\n",
    "          validation_data=(validation_inputs, validation_targets), # validation data\n",
    "          verbose = 2 # making sure we get enough information about the training process\n",
    "          )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model\n",
    "\n",
    "After training on the training data and validating the validation data, we tested the final predictive power of our model by running it on the test data set that the algorithm has never seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448/448 [==============================] - 0s 383us/sample - loss: 0.2408 - accuracy: 0.9129\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test loss: 0.24. Test accuracy: 91.29%\n"
     ]
    }
   ],
   "source": [
    "print('\\nTest loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the initial model and hyperparameters given in this notebook, the final test accuracy should be roughly around 91%.\n",
    "\n",
    "Note that each time the code is rerun, we get a different accuracy because each training is different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
